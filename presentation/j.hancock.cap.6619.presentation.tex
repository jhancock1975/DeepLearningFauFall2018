%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
%\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{url}
\usepackage{listings}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Unsupervised Representation Learning with Deep Convolutional
Generative Adversarial Networks]{Presentation on ``Unsupervised Representation 
Learning with Deep Convolutional Generative Adversarial Networks,'' by Alec Radford, Luke Metz, and Soumith Chintala} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{John Hancock} % Your name
\institute[FAU] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Florida Atlantic University \\ % Your institution for the title page
\medskip
\textit{jhancoc4@fau.edu} % Your email address
}
\date{\today} % Date, can be changed to a custom date

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Overview} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation 
\end{frame}

%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------
\section{Introduction}
\begin{frame}
\frametitle{Introduction}
In the field of deep learning, we have crossed into an era where we are experimenting 
with designs that involve more than one neural network.  

Generative adversarial networks are a design pattern to employ two neural networks.  

``Unsupervised representational learning with deep convolutional generative 
adversarial networks''  \cite{repLearnDcgan}, is a milestone in the development of 
Generative Adversarial Networks where the authors report a reliable architecture 
that incorporates convolutional neural networks into the generative adversarial 
network design.  


The authors tout several applications of their design to prove its utility.

For the remainder of this presentation, we will refer to the paper entitled , 
``Unsupervised representational learning with deep convolutional generative 
adversarial networks,'' as,  ``the DCGAN's paper,'' or by its reference number
\cite{repLearnDcgan}.

\end{frame}

%------------------------------------------------


%------------------------------------------------
\section{Background} 
% Sections can be created in order to organize your presentation into discrete blocks, all sections and subsections are automatically printed in the table of contents as an overview of the talk
%------------------------------------------------
\begin{frame}
\frametitle{Background}
A generative adversarial network (GAN) is a neural network with two components. 
Goodfellow \textit{et. al} invent GAN's in \cite{gan}. To a first approximation,
GAN's work as follows:
\begin{itemize}
  \item The first component is a \textit{generator} that learns to transform vectors 
    of random numbers into output values that resemble instances from some dataset.  
  \item The second component is a \textit{discriminator} that classifies things into 
  two categories:
  \begin{itemize}
    \item the class of instances of the dataset, and
    \item the class of generator outputs.
  \end{itemize}
  \item ``At convergence, the generatorâ€™s samples are indistinguishable from real data,
     and the discriminator outputs $\frac{1}{2}$ everywhere. The discriminator may 
     then be discarded" \cite{deepLearnBookGenCh}. from \underline{Deep learning} , 
     Goodfellow \textit{et al.} 
\end{itemize}
\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Background}
\begin{itemize}

\item ... or not.  The authors of the DCGAN paper find a use for the discriminator.
  \item In the context of this paper, the outputs are images.  However, 
   researchers use GAN's where the generators create other artifacts. We find
   an extensive list on Github  \cite{ganList} of over 500 research projects. 
   Some examples from this list are:
  \begin{itemize}
    \item imputing missing values in datasets,
    \item generating music,
    \item fraud detection, and
    \item playing chess.
  \end{itemize}
\end{itemize}
\end{frame}

%------------------------------------------------

\section{Contributions}
\begin{frame}
\frametitle{Contributions}

The authors of the paper make several contributions they...
\begin{itemize}
  \item invent an architecture for DCGAN's,
  \item use the convolutional layer filters of trained DCGAN's discriminators as 
    feature extractors for doing classifications,
  \item demonstrate that after training the DCGAN, its filters learn how to
    represent images, and
  \item present a method of doing vector arithmetic using DCGAN inputs to do 
    inferences \emph{{\`a} la} Word2Vec \cite{word2Vec}.
\end{itemize}
\end{frame}

%------------------------------------------------
\section{Architecture}
\begin{frame}
\frametitle{Architecture}
A high-level overview of the architecture:
\begin{itemize}
  \item they use convolutions
  \item they use de-convolutions
  \item subsequent work such as the texts for this course 
   \cite{deepLearnR} \cite{deepLearnBookGenCh} suggests one
   should employ dropout, but we do not find that the authors of this paper use it
   when inspecting the code in \cite{dcganCode}.
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Architecture}
subsection{Deconvolution visualization}
The term deconvolution seems to have stuck because we see library functions in

The authors of the paper prefer the term, ``fractionally-strided.'' We find another
author who gives the name, ``

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Datasets}
\begin{itemize}
  \item The authors use three datasets for training:
  \begin{itemize}
    \item Large Scale Scene Understanding (LSUN),
    \item Imagenet 1-K, and
    \item Faces.
  \end{itemize}  
  \item The authors two datasets for evaluating unsupervised learning:
  \begin{itemize}
    \item Canadian Institute for Advanced Research (CIFAR) 10
    \item StreetView House Numbers (SVHN)
  \end{itemize}
  \item Note: the authors mention that they heuristically removed duplicate
    images from LSUN to prevent the DCGAN from memorizing images.
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{LSUN}
\begin{itemize}
 \item The authors used images of bedrooms from the LSUN dataset \cite{lsunDataset} as input to their model.
\end{itemize}
\end{frame}

%------------------------------------------------

\section{Implementation Details}
\begin{frame}
\frametitle{How did they do it?}
The code for this paper, as well as many others in the references and
that one may find in the course of research, is on Github in the dcgan\_code
project \cite{dcganCode}.

This code is a bit outdated, however the dcgan\_code Github project has a link
to the DCGAN-tensorflow \cite{dcganTf} project that we find more accessible.
\end{frame}

%------------------------------------------------


\begin{frame}
\frametitle{Fractionally-strided convolutions}
A deep convolutional generative adversarial network (DCGAN) is a GAN that uses
convolutional neural networks for the generator and discriminator.

The discriminator uses convolutional layers in the sense we are familiar with, such
as the convolutional layers LeCun describes in LeNet-5 \cite{lenet5}.

However, the generator uses convolutional layers that we find called deconvolutional
layers in the source code that accompanies this paper \cite{dcganCode}, and elsewhere,
but that in the paper the authors write that we should prefer the term ``\textit{fractionally-strided}.''  The computations that comprise fractionally-strided convolutions
are not clear to us from the paper or the source code that accompanies it.  We find
the source code unclear because the authors implement fractionally-strided 
convolutions using library functions, the source code of which we run out of time
to peruse.  The paper lacks detail on how to compute a fractionally-strided 
convolutions.
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Fractionally-strided convolutions}
On the other hand, the github project \cite{dcganCode} that accompanies the paper
\cite{repLearnDcgan} links to a Tensorflow implementation of the same code:
\cite{dcganTf} where the author of this code implements fractionally-strided 
convolutions using Tensorflow's conv2d\_transpose.  We did some internet searching
and found \cite{convArith}.   This reference plus using conv2d\_transpose in a
small example helps us understand precisely how the fractionally-strided convolution
operation works.  We feel confident to rely on conv2d\_transpose because the 
authors of the paper \cite{repLearnDcgan} we review here provide a link to the
code in \cite{dcganTf} in their own code. We feel the authors of the DCGAN's paper's
endorsement of the Tensorflow code means conv2d\_transpose is a valid method for
doing what the authors of \cite{repLearnDcgan} refer to as fractionally-strided
convolutions, and that a good understanding of conv2d\_transpose is a good
understanding of fractionally-strided convolutions.

\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Fractionally-strided convolutions}
We found some example code, and a great diagram from a StackExchange.com discussion 
\cite{stackExConv}. That explains in detail how conv2d\_transpose works.
This is the diagram we found:

\includegraphics[scale=0.25] {conv2d_transpose-example}


In the interest of time we will not go over the code for the example, but we
will give some highlights.

\end{frame}

%-----------------------------------------------
\begin{frame}
\frametitle{Fractionally-strided convolutions}
\begin{itemize}
\item Since anyone might write anything in online discussion forums, we decided to 
confirm that Tensorflow's conv2d\_transpose operation works as the digram implies.
conv2d\_transpose has three important parameters: input tensor, 
filter, and stride. 

\item One should be careful not to confuse the term filter we
have for conv2d\_transpose and the filters that the authors of the DCGAn's paper
show on page 9. 

\item In the context of the DCGAN paper it is better to think of the
filter parameter of conv2d\_transpose  as a kernel for the conv2d\_transpose operation,
and the filter is the result of applying conv2d\_transpose to the input tensor. 
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Fractionally-strided convolutions}
\begin{itemize}

\item The next slide shows  a 4x4  input tensor, and the result of applying 
conv2d\_transpose to that tensor, with stride of 1,4,4,1. 

\item Conv2d\_transpose operates on 4 dimensional tensors, so we must embed the  4x4
matrix in a 4-dimensional tensor, and stride through it accordingly. Note on the 
next slide how most entries in the output tensor are copies of entries in the input
tensor, except where the 5x5 kernels must overlap in order to achieve the 
16x16 output.
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Fractionally-strided convolutions}
The input tensor:
\[
\begin{bmatrix}
  1 & 2 & 3 & 4 \\
  5 & 6 & 7 & 8 \\ 
  9 & 10 & 11 & 12 \\
  13 & 14 & 15 & 16
\end{bmatrix}
\]
The output we can see the entries in the input matrix copied into 5x5 intermediate
tensors and then added according to the stride of 4, and values are added when
we have overlap.  We show a screen shot to prove the code runs.
\includegraphics[scale=0.25]{conv2d-result}
\end{frame}

%------------------------------------------------

\section{Run Tensorflow Implementation MNIST Example}
\begin{frame}
\frametitle{Results}
The authors of the paper make several contributions:
Here reference github code implementation
How to run their mnist example:
\begin{itemize}
  \item Use AWS Ubuntu Deep Learning Instance
  \item \begin{itemize}
    \item Expensive $\approx$ \$0.65 per hour!
    \item Configure an alarm to shut the instance down after 3 hours!
    \end{itemize}
 \item Create virtual environment
 \item \begin{itemize}
    \item Use pip to install libraries
    \item force install of Theano 0.9.0 (pip install -I Theano 0.9.0)
    \end{itemize} 
 \item \begin{itemize}
    \item Paper dcgan\_code repository does not have MNIST data,
    \item Download MNIST data from \url{https://github.com/Manuel4131/GoMNIST/tree/master/data}, and change location in lib/config.py
    \end{itemize}
 \end{itemize}
\end{frame}

%------------------------------------------------
% https://ieee-dataport.org/sites/default/files/analysis/27/IEEE%20Citation%20Guidelines.pdf

\begin{frame}[allowframebreaks]
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99} % Beamer does not support BibTeX so references must be inserted manually as below

\bibitem{repLearnDcgan} S. Chintala, L. Metz, and A. Radford, 
Unsupervised representation learning with deep convolutional generative 
adversarial networks.  2016. [Online]. Available: arXiv:1511.06434v2 [cs.LG]

\bibitem{dcganCode} S. Chintala, L. Metz, and A. Radford, dcgan\_code. (2016,
May).  Available: \url{https://github.com/Newmu/dcgan\_code}. [Accessed Nov. 19,
2018].

\bibitem{lsunDataset} Large Scale Scene Understanding Challenge (2017, July).
Available: \url{http://lsun.cs.princeton.edu/2017/}. [Accessed Nov. 19, 2018].

\bibitem{dcganTf} T Kim. (2018, August).  Available: \url{https://github.com/carpedm20/DCGAN-tensorflow}. [Accessed Nov. 19, 2018].


\bibitem{gan} Y. Bengio, A. Courville,I. Goodfellow, M. Mirza, S. Ozair,  
J. Pouget-Abadie,  D. Warde-Farley, B. Xu, Generative adversarial nets. 2014. 
[Online]. Available: arXiv:1406.2661v1 [stat.ML]

\bibitem{word2Vec} T. Mikolov, I. Sutskever, K. Chen, G. Corrado, J. Dean,
``Distributed Representations of Words and Phrases and their Compositionality,''
Advances in Neural Information Processing Systems 26 (NIPS 2013), 2013. 
[Online] Available: \url{http://papers.nips.cc/paper/5021-distributed-}representations-of-words-and-phrases-and-their-compositionality.pdf

\bibitem{slidetemplate} Creodocs Limited,``Beamer Presentation,''
\emph{latextemplates.com}, 2018. [Online], Available: 
\url{http://www.latextemplates.com/templates/presentations/1/presentation\_1.zip}. [Accessed Nov. 10, 2018].

\bibitem{ieeeStyle} IEEE, Piscataway, NJ, USA. \emph{IEEE Editorial Style Manual}. 2016.
[Online]. Available: \url{http://ieeeauthorcenter.ieee.org/wp-content/uploads/IEEE\_Style_Manual.pdf}, [Accessed Nov. 11, 2018].

\bibitem{unsupVideo} Y. LeCun, ``Unsupervised Representation Learning,''
2017. Accessed on: Nov 11, 2018. [online]. Available: \url{https://www.youtube.com/watch?v=ceD736_Fknc}  

\bibitem{deepLearnR} F. Chollet, and J.J. Allaire.  
\textit{Deep Learning with R}. Manning Publications 2018. [E-Book] Available: Safari
E-Book.

\bibitem{deepLearnBookGenCh} Y. Bengio, A. Courville, I. Goodfellow. (2016). ``Chapter 20 Deep Generative Models,'' 2016. [Online] Available: \url{https://www.deeplearningbook.org/contents/generative_models.html}. [Accessed: Nov. 20, 2018].

\bibitem{ganList} A. Hindupur, ``A list of all named GANs!'' github.com, Sep. 30,
2018. [Online]. Available: \url{https://github.com/hindupuravinash/the-gan-zoo}. 
[Accessed Nov. 21, 2018]. 

\bibitem{lenet5} L. Bottou, P. Haffner, Y. Bengio, Y. LeCun, ``Gradient-Based Learning
Applied to Document Recognition,'' Proc of the IEEE, Nov., 1998. [Online]. Available:
\url{http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf}. [Accessed Nov. 21, 2018].

\bibitem{convArith} V. Dumoulin and F. Visin, ``A guide to convolution arithmetic for 
deep learning,'' Jan 2018. [Online]. Available: Github, https://github.com/vdumoulin/conv\_arithmetic.  [Accessed November 21, 2018].

\bibitem{stackExConv} Stack Exchange user Andriys, ``What are deconvolutional layers,''
datascience.stackexchange.com, answer written July 14 2017, edited Nov. 19, 2017. 
[Online]. Available: 
\url{https://datascience.stackexchange.com/questions/6107/what-are-deconvolutional-layers}. 
[Accessed Nov.  22, 2018].
\end{thebibliography}
}
\end{frame}

%------------------------------------------------

\begin{frame}
\Huge{\centerline{The End}}
\end{frame}

%----------------------------------------------------------------------------------------

\end{document} 
